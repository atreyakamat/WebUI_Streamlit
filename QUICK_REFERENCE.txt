╔═══════════════════════════════════════════════════════════════════════════╗
║                       OLLAMA UI - QUICK REFERENCE                         ║
╚═══════════════════════════════════════════════════════════════════════════╝

┌─────────────────────────────────────────────────────────────────────────┐
│ QUICK START (3 Steps)                                                   │
└─────────────────────────────────────────────────────────────────────────┘

1. Install dependencies:
   pip install -r requirements.txt

2. Start the application:
   START_APP.bat

   OR manually:
   Terminal 1: python backend.py
   Terminal 2: streamlit run app_unified.py

3. Open browser: http://localhost:8501

┌─────────────────────────────────────────────────────────────────────────┐
│ OLLAMA COMMANDS                                                         │
└─────────────────────────────────────────────────────────────────────────┘

Check version:       ollama --version
List models:         ollama list
Pull model:          ollama pull llama3.2
Run model:           ollama run llama3.2
Remove model:        ollama rm llama3.2
Show model info:     ollama show llama3.2

┌─────────────────────────────────────────────────────────────────────────┐
│ RECOMMENDED MODELS                                                      │
└─────────────────────────────────────────────────────────────────────────┘

llama3.2 (4.7 GB)    - Fastest, general purpose
mistral (4.1 GB)     - Good balance, creative
codellama (3.8 GB)   - Best for coding tasks
llama3.1 (8.0 GB)    - Most capable, slower

┌─────────────────────────────────────────────────────────────────────────┐
│ KEYBOARD SHORTCUTS                                                      │
└─────────────────────────────────────────────────────────────────────────┘

Toggle Sidebar:      Click ☰ button
New Chat:            Ctrl+R (reload page)
Focus Input:         Click chat input at bottom
Send Message:        Enter (in input field)
Search Chats:        Click search bar in sidebar

┌─────────────────────────────────────────────────────────────────────────┐
│ TROUBLESHOOTING                                                         │
└─────────────────────────────────────────────────────────────────────────┘

Problem                          Solution
─────────────────────────────────────────────────────────────────────────
Backend connection failed        • Check backend.py is running
                                 • Visit: http://localhost:8000/health

Ollama not found                 • Start Ollama application
                                 • Visit: http://localhost:11434

Model not available              • Run: ollama pull llama3.2
                                 • Check: ollama list

Sidebar not visible              • Click ☰ button (top-left)
                                 • Refresh browser (F5)

Slow responses                   • Use llama3.2 (fastest)
                                 • Lower temperature in settings
                                 • Close other applications

┌─────────────────────────────────────────────────────────────────────────┐
│ API ENDPOINTS                                                           │
└─────────────────────────────────────────────────────────────────────────┘

Backend Root:        http://localhost:8000
Health Check:        http://localhost:8000/health
List Models:         http://localhost:8000/models
Chat API:            http://localhost:8000/api/chat (POST)

Ollama:              http://localhost:11434

┌─────────────────────────────────────────────────────────────────────────┐
│ FEATURES                                                                │
└─────────────────────────────────────────────────────────────────────────┘

✓ Multiple chat threads
✓ Auto-generated chat titles
✓ Search conversations by content
✓ Delete individual chats
✓ Model selection (llama3.2, mistral, codellama)
✓ Temperature control
✓ Streaming responses
✓ Dark theme UI
✓ Sidebar toggle
✓ Persistent conversations (session-based)

┌─────────────────────────────────────────────────────────────────────────┐
│ FILE STRUCTURE                                                          │
└─────────────────────────────────────────────────────────────────────────┘

app_unified.py       - Main Streamlit UI
backend.py           - FastAPI backend server
requirements.txt     - Python dependencies
SETUP_GUIDE.txt      - Detailed setup instructions
START_APP.bat        - Quick start script
QUICK_REFERENCE.txt  - This file

┌─────────────────────────────────────────────────────────────────────────┐
│ SUPPORT                                                                 │
└─────────────────────────────────────────────────────────────────────────┘

Ollama: https://ollama.ai
Docs:   https://github.com/ollama/ollama
Issues: https://github.com/ollama/ollama/issues

═══════════════════════════════════════════════════════════════════════════
